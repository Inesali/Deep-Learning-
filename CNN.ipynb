{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir= r\"C:\\Users\\inesa\\Desktop\\Cours\\Semestre 3\\DL\\arabic_handwritten_data\\data\\train_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir=r\"C:\\Users\\inesa\\Desktop\\Cours\\Semestre 3\\DL\\arabic_handwritten_data\\data\\test_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movefile(srcDir, dstDir, i ):\n",
    "    # Check if both the are directories\n",
    "    if os.path.isdir(srcDir) and os.path.isdir(dstDir) :\n",
    "        # Iterate over all the files in source directory\n",
    "        for filePath in glob.glob(srcDir + '\\*_label_'+ str(i+1)+'.png'):\n",
    "            # Move each file to destination Directory\n",
    "            shutil.move(filePath, dstDir);\n",
    "    else:\n",
    "        print(\"srcDir & dstDir should be Directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(arabic_mnist_labels)):\n",
    "    directory= arabic_mnist_labels[i]\n",
    "    path= os.path.join(train_dir, directory)\n",
    "    path2= os.path.join(test_dir , directory)\n",
    "    os.mkdir(path) \n",
    "    os.mkdir(path2)\n",
    "    movefile(train_dir,path,i)  \n",
    "    movefile(test_dir,path2,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training rock images: 480\n"
     ]
    }
   ],
   "source": [
    "alef=\"alef\"\n",
    "alef_dir=os.path.join(train_dir,alef)\n",
    "print('total training rock images:', len(os.listdir(alef_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training rock images: 480\n"
     ]
    }
   ],
   "source": [
    "beh=\"beh\"\n",
    "beh_dir=os.path.join(train_dir,beh)\n",
    "print('total training rock images:', len(os.listdir(beh_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "pic_index = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alef_files = os.listdir(alef_dir)\n",
    "\n",
    "beh_files = os.listdir(beh_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAADOElEQVR4nO3d0U2DYBSAUWicwilcwjiBUzqBcQmncAzpAoQHUuCjnPPYpgkvX27CLfzjNE0D0HM7+gKAeeKEKHFClDghSpwQ9bL05fvt061c2NjP/9c497nJCVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUYsnW9P0/fe76ncfr28PvQ62ZXJClDghSpwQJU6IEidEiROirFKi1q5LeB4mJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKH98v5BH/5neO4m2ZXJClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0R5KmUHjlZgDZMTosQJUeKEKHFClDghSpwQZZXCMAxe1lVkckKUOCFKnBAlTogSJ0SJE6KsUnawtKbY84kV65JzMTkhSpwQJU6IEidEiROixAlRVikHW7PeWLt+WfqdNUuPyQlR4oQocUKUOCFKnBDlbm2UIxwwOSFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTohyVkrU0knTzlG5BpMTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUY5jOCFHNVyDyQlR4oQocUKUOCFKnBAlToiySnkyS2sWzsXkhChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQNU7TdPQ1ADNMTogSJ0SJE6LECVHihChxQtQdbkQp35suGx8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAADU0lEQVR4nO3d0UnDYBhAUVucwilcQpzAKZ1AXMIpHMP0vbQBY23un5zzqATycvkgX//kME3TA9BzXPsGgMvECVHihChxQpQ4Iepx7p8vxzePcuGfff68Hy793eSEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oSo2S9bM56P769F170+Pd/0Pvg7kxOixAlR4oQocUKUOCFKnBBllRK1dCXCdpicECVOiBInRIkTosQJUeKEKKuUld1zZeLkyVhMTogSJ0SJE6LECVHihChPazfGE9ntMDkhSpwQJU6IEidEiROixAlRVikru7b6WPqD+LnrrFnGYnJClDghSpwQJU6IEidEiROirFJ2xJplLCYnRIkTosQJUeKEKHFClDghyirlF3xtmnsyOSFKnBAlTogSJ0SJE6LECVFWKWesS6gwOSFKnBAlTogSJ0SJE6LECVFWKTviJV5jMTkhSpwQJU6IEidEiROiPK0944kmFSYnRIkTosQJUeKEKHFClDghyiplZbd+Z5FV0HaYnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUU6lrOzaKZKlp1XmrnNiZSwmJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR3iEUNfe+n1t/DZsmkxOixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IcqplB3x1euxmJwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRHmH0IC872cfTE6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDgh6jBN09r3AFxgckKUOCFKnBAlTogSJ0SJE6JOKnkzeFiN6gMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAADWklEQVR4nO3d0UnDABhGUVs6hVO4hDiBUzqBuIRTOIbxvdRYYpreJOc8KpS8XH7IV/EwDMMD0HO89wMAl4kTosQJUeKEKHFC1Gnsl8/HV69y4cY+vt8Ol37uckKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQdbr3A7BN71+fs37ey+PTrJ+3Bi4nRIkTosQJUeKEKHFClDghypRyZu4J4C97nAi4jssJUeKEKHFClDghSpwQJU6I2uWUsvRcMua3ZzGx4HJClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlToja5Rffl7bVL7GX/oBgi1xOiBInRIkTosQJUeKEKHFC1C6nlLFp4xbzwBr+y3NlFtnq7DSFywlR4oQocUKUOCFKnBAlToja5ZSydpXZYypzyXVcTogSJ0SJE6LECVHihChxQpQp5cyW/+JjSeaS/3M5IUqcECVOiBInRIkTosQJUaaUBcw9K9ximjF99LicECVOiBInRIkTosQJUd7WLmDK29Wxt6ferO6DywlR4oQocUKUOCFKnBAlTogypUSNzS+mlH1wOSFKnBAlTogSJ0SJE6LECVGmlBUys+yDywlR4oQocUKUOCFKnBAlTogypSzAvMEULidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUYdhGO79DMAFLidEiROixAlR4oQocUKUOCHqB3K5N5RDqunqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAADRUlEQVR4nO3d0U3CYBhAUSFM4RQuYZzAKZ3AuIRTOIZlASAmlL/3t+c8QlL6cvMl/Vp6WJblCeg5bn0CwGXihChxQpQ4IUqcEHW69eXr8d2lXHiwr9+Pw6XPTU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTok5bnwDz+vz5Xv2Yb88vqx9zViYnRIkTosQJUeKEKHFClDghyiplgLVXDtYN+2ByQpQ4IUqcECVOiBInRIkToqxSVvKIJzTYN5MTosQJUeKEKHFClDghSpwQZZUyoVtrm5FPrNz6Laul+5mcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROi3Pj+z4y8Kd7N7Y9lckKUOCFKnBAlTogSJ0SJE6KsUlYyw//pVM6DvzE5IUqcECVOiBInRIkTosQJUVYpA8ywZhlp5CsjZmZyQpQ4IUqcECVOiBInRIkToqxSNmatwDUmJ0SJE6LECVHihChxQpSrtRsbeeO7K8NzMTkhSpwQJU6IEidEiROixAlRVik7MvKt19zP5IQocUKUOCFKnBAlTogSJ0RZpeyIdclcTE6IEidEiROixAlR4oQocUKUVcrGrDe4xuSEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IeqwLMvW5wBcYHJClDghSpwQJU6IEidEiROizjw8MHJogMM7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "next_alef = [os.path.join(alef_dir, fname) \n",
    "                for fname in alef_files[pic_index-2:pic_index]]\n",
    "next_beh = [os.path.join(beh_dir, fname) \n",
    "                for fname in beh_files[pic_index-2:pic_index]]\n",
    "\n",
    "for i, img_path in enumerate(next_alef+next_beh):\n",
    "  #print(img_path)\n",
    "  img = mpimg.imread(img_path)\n",
    "  plt.imshow(img)\n",
    "  plt.axis('Off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_preprocessing\n",
    "from keras_preprocessing import image\n",
    "from keras_preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13440 images belonging to 28 classes.\n",
      "Found 3360 images belonging to 28 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR = \"C:/Users/inesa/Desktop/Cours/Semestre 3/DL/arabic_handwritten_data/data/train_data/\"\n",
    "training_datagen = ImageDataGenerator(\n",
    "      rescale = 1./255,\n",
    "\t    rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "VALIDATION_DIR = \"C:/Users/inesa/Desktop/Cours/Semestre 3/DL/arabic_handwritten_data/data/test_data/\"\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "train_generator = training_datagen.flow_from_directory(\n",
    "\tTRAINING_DIR,\n",
    "\ttarget_size=(150,150),\n",
    "\tclass_mode='categorical',\n",
    "  batch_size=126\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "\tVALIDATION_DIR,\n",
    "\ttarget_size=(150,150),\n",
    "\tclass_mode='categorical',\n",
    "  batch_size=126\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 148, 148, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 74, 74, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 72, 72, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 28)                14364     \n",
      "=================================================================\n",
      "Total params: 3,486,300\n",
      "Trainable params: 3,486,300\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 20 steps, validate for 3 steps\n",
      "Epoch 1/25\n",
      "20/20 [==============================] - 145s 7s/step - loss: 3.3643 - accuracy: 0.0417 - val_loss: 3.2768 - val_accuracy: 0.0265\n",
      "Epoch 2/25\n",
      "20/20 [==============================] - 148s 7s/step - loss: 3.2772 - accuracy: 0.0631 - val_loss: 3.1290 - val_accuracy: 0.1032\n",
      "Epoch 3/25\n",
      "20/20 [==============================] - 139s 7s/step - loss: 3.1743 - accuracy: 0.0964 - val_loss: 2.9673 - val_accuracy: 0.1402\n",
      "Epoch 4/25\n",
      "20/20 [==============================] - 158s 8s/step - loss: 3.0752 - accuracy: 0.1127 - val_loss: 2.7109 - val_accuracy: 0.2169\n",
      "Epoch 5/25\n",
      "20/20 [==============================] - 158s 8s/step - loss: 2.9523 - accuracy: 0.1417 - val_loss: 2.6151 - val_accuracy: 0.2328\n",
      "Epoch 6/25\n",
      "20/20 [==============================] - 163s 8s/step - loss: 2.8802 - accuracy: 0.1643 - val_loss: 2.2776 - val_accuracy: 0.3519\n",
      "Epoch 7/25\n",
      "20/20 [==============================] - 152s 8s/step - loss: 2.7801 - accuracy: 0.2012 - val_loss: 2.2624 - val_accuracy: 0.3386\n",
      "Epoch 8/25\n",
      "20/20 [==============================] - 157s 8s/step - loss: 2.6773 - accuracy: 0.2218 - val_loss: 2.0393 - val_accuracy: 0.4048\n",
      "Epoch 9/25\n",
      "20/20 [==============================] - 162s 8s/step - loss: 2.5374 - accuracy: 0.2462 - val_loss: 1.8366 - val_accuracy: 0.4577\n",
      "Epoch 10/25\n",
      "12/20 [=================>............] - ETA: 1:21 - loss: 2.4267 - accuracy: 0.2796"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
    "    # This is the first convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150,3 )),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # The second convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The third convolution\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The fourth convolution\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(28, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_generator, epochs=25, steps_per_epoch=20, validation_data = validation_generator, verbose = 1, validation_steps=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = get_dataset(train_dir).shuffle(buffer_size=batch_size*10).batch(batch_size)\n",
    "valid_ds = get_dataset(test_dir).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, epochs=25, steps_per_epoch=20, validation_data = valid_ds, verbose = 1, validation_steps=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from google.colab import files\n",
    "from keras.preprocessing import image\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    " \n",
    "  # predicting images\n",
    "  path = fn\n",
    "  img = image.load_img(path, target_size=(150, 150))\n",
    "  x = image.img_to_array(img)\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "\n",
    "  images = np.vstack([x])\n",
    "  classes = model.predict(images, batch_size=10)\n",
    "  print(fn)\n",
    "  print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
